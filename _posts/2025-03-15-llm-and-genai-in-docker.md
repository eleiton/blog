---
title: Running LLM and GenAI on Intel Arc GPU
description: Make use of Intel Arc Series GPU to run Open WebUI with Ollama to interact with Large Language Models (LLM) and Generative AI (GenAI)
author: eleiton
date: 2025-03-15 00:00:00 +0100
categories: [AI]
tags: [GenAI,LLM]
pin: true
math: true
mermaid: true
image:
  path: /assets/img/2025-03-15/tucan.jpg
  alt: Picture of a tucan generated with GenAi
---
## Introduction
Have you ever dreamed of having a personal AI companion that can assist with tasks, generate ideas, or even create art? Imagine being able to bring your creative visions to life with ease, leveraging the power of artificial intelligence to streamline your workflow and enhance your productivity.

As a software engineer, you're likely no stranger to the world of code and innovation. But have you ever considered taking your AI skills to the next level by running your own chatbots, generative AI systems, or content generation tools? With the rise of deep learning and natural language processing, the possibilities are endless, and the potential for creativity and automation is vast.

While online services like ChatGTP and Perplexity offer a glimpse into the world of AI development, they also present limitations. Running your own AI systems allows you to tailor solutions to your specific needs, integrate with existing tools, keep your privacy, and push the boundaries of what's possible.

After reading this post, you'll understand how to:

- [x] Seamlessly deploy a single Docker-based solution that integrates all necessary components
- [x] Leverage the user-friendly interface of Open WebUI for easy model management and deployment
- [x] Unlock the full potential of Large Language Models (LLM) with Ollama's advanced integration capabilities
- [x] Streamline Stable Diffusion capabilities with SD.Next's optimization tools
- [x] Maximize performance and efficiency using Intel Arc Series GPUs and Intel Extension for PyTorch

## TL;DR
For developers who prefer to dive straight into the code, I've got you covered! 
The docker-based deployment solution is available on GitHub. Simply click [this link](https://github.com/eleiton/ollama-intel-arc) to access the code repository and start building your own AI-powered workflow today.


## References
GitHub Repository: <https://github.com/eleiton/ollama-intel-arc>
